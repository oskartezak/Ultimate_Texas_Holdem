{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN with embedded encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size 8 embedding with padding\n",
    "\n",
    "layers: [57] -> [64, 32] -> 2\n",
    "\n",
    "Batch size 32\n",
    "\n",
    "Just CPU as its faster for smaller models, no data transfer\n",
    "\n",
    "Implement Prioritized Experience Replay\n",
    "\n",
    "Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'NN_functions' from 'c:\\\\Users\\\\neomi\\\\OneDrive\\\\Desktop\\\\PDF\\\\Poker Ultimate\\\\Ultimate_Texas_Holdem\\\\NN-Neo_igranje\\\\NN_functions.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random \n",
    "import ultimate_for_NN as ultimate\n",
    "from collections import deque\n",
    "import NN_functions as NN\n",
    "import time\n",
    "from importlib import reload\n",
    "reload(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make model\n",
    "class DQNEmbedding(nn.Module):\n",
    "    def __init__(self, num_cards, embedding_dim, hidden_1, hidden_2, output_dim):\n",
    "        super(DQNEmbedding, self).__init__()\n",
    "\n",
    "        # Embedding layer: 52 cards + 1 padding token\n",
    "        self.card_embedding = nn.Embedding(num_cards + 1, embedding_dim, padding_idx=0)\n",
    "\n",
    "        self.fc1 = nn.Linear(embedding_dim * 7 + 1, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, output_dim)\n",
    "    \n",
    "    def forward(self, card_indices, game_round):\n",
    "\n",
    "        # Convert card indices into embeddings\n",
    "        embedded_cards = self.card_embedding(card_indices)  # Shape: (batch, 5, 8)\n",
    "\n",
    "        # Flatten embeddings\n",
    "        flat_cards = embedded_cards.view(embedded_cards.size(0), -1)  # Shape: (batch, 40)\n",
    "        #print(flat_cards.shape, game_round.shape, game_round.unsqueeze(1).shape)\n",
    "        # Combine with game state\n",
    "        x = torch.cat([flat_cards,  game_round], dim=1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for training the model\n",
    "def train_model(model, target_model, optimizer, loss_fn, num_of_games, buffer, \n",
    "                EPSILON = .2, starting_round = 0):\n",
    "    batch_size = 32\n",
    "    target_update = 500\n",
    "    train_freq = 1\n",
    "    GAMMA = .9\n",
    "\n",
    "    def update_model_weights():\n",
    "        if buffer.size() < batch_size:  \n",
    "            return\n",
    "        \n",
    "        # Sample a mini-batch\n",
    "        actions, rewards, cards_input_tensors, round_input_tensors, next_cards_input_tensors, next_round_input_tensors, end = buffer.sample(batch_size)\n",
    "    \n",
    "        # Convert to PyTorch tensors\n",
    "        actions = torch.tensor(actions, dtype=torch.long)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        cards_input_tensors = torch.tensor(cards_input_tensors, dtype=torch.long)\n",
    "        round_input_tensors = torch.tensor(round_input_tensors, dtype=torch.long).squeeze(1)\n",
    "        next_cards_input_tensors = torch.tensor(next_cards_input_tensors, dtype=torch.long)\n",
    "        next_round_input_tensors = torch.tensor(next_round_input_tensors, dtype=torch.long).squeeze(1)\n",
    "        end = torch.tensor(end, dtype=torch.float32)\n",
    "        # Compute Q-values for current states (only the taken actions)\n",
    "        q_values = model(cards_input_tensors, round_input_tensors).squeeze(1).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute target Q-values using Bellman equation\n",
    "        target_q_values = rewards.clone()\n",
    "        with torch.no_grad():\n",
    "            next_q_values = target_model(next_cards_input_tensors, next_round_input_tensors).squeeze(1).max(1)[0]  # Max Q-value for next state\n",
    "            target_q_values += (GAMMA * next_q_values * (1 - end))  # Q-learning update, if round ended no future is included\n",
    "\n",
    "        # Compute loss (Mean Squared Error loss)\n",
    "        loss = loss_fn(q_values, target_q_values)\n",
    "\n",
    "        # Backpropagation & gradient update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Set model to train mode\n",
    "    model.train()\n",
    "\n",
    "    # Copy weights from dqn to target_model\n",
    "    target_model.load_state_dict(model.state_dict())\n",
    "    target_model.eval()  # No gradient updates for target\n",
    "\n",
    "    for episode in range(num_of_games):\n",
    "\n",
    "        if episode % target_update == 0:\n",
    "            target_model.load_state_dict(model.state_dict())  # Sync weights for target model\n",
    "        \n",
    "        if episode % train_freq == 0:  # Train every train_freq steps\n",
    "            update_model_weights()\n",
    "\n",
    "        round = starting_round  # Start at Round 1\n",
    "        done = False\n",
    "        \n",
    "        # generate a training game\n",
    "        whole_game = NN.generate_game()\n",
    "\n",
    "        while not done:\n",
    "            # get input data for this round\n",
    "            cards_input_tensor, round_input_tensor = NN.state_to_tensor_embedding(round, whole_game)\n",
    "\n",
    "            end = 0\n",
    "\n",
    "            # Epsilon-greedy action selection, we will explore with probability EPSILON\n",
    "            if np.random.rand() < EPSILON:\n",
    "                action = np.random.choice([0, 1])\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values = model(cards_input_tensor, round_input_tensor)\n",
    "                    action = q_values.argmax().item()\n",
    "\n",
    "            # Calculate reward based on state and action, if action is check, reward is 0\n",
    "            reward = NN.reward_function(round, whole_game, action)  \n",
    "            \n",
    "            if action == 1:\n",
    "                round += 1 # move to the next state if the action is check/fold\n",
    "            \n",
    "            if action == 0 or round == 3:\n",
    "                end = 1\n",
    "\n",
    "            # TODO: get expected reward and save game only if it deviates a lot from the q value\n",
    "                \n",
    "            # Determine the next state, if state == 3 it will be irrelevant\n",
    "            next_cards_input_tensor, next_round_input_tensor = NN.state_to_tensor_embedding(round, whole_game)\n",
    "            \n",
    "            # add game to buffer\n",
    "            buffer.add(action, reward, cards_input_tensor, round_input_tensor, \n",
    "                       next_cards_input_tensor, next_round_input_tensor, end)\n",
    "           \n",
    "            # Transition to next state or end the episode if terminal\n",
    "            if  round == 3 or action == 0:\n",
    "                done = True\n",
    "\n",
    "        #if episode % 100 == 0:\n",
    "        #    print(f\"Episode {episode}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the model - layer dimensions\n",
    "num_cards = 52\n",
    "embedding = 8  # 1 for game state [0, 1, 2] and 7 for cards, -1 means not known\n",
    "hidden_1 = 64 # hidden layer 1 size\n",
    "hidden_2 = 32 # hidden layer 2 size\n",
    "output_dim = 2  # Two actions: Bet (0) and Check/Fold (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DQN\n",
    "emb_model = DQNEmbedding(num_cards, embedding, hidden_1, hidden_2, output_dim)\n",
    "# Initialize DQN for target rewards, it will lag behind \n",
    "emb_target_model = DQNEmbedding(num_cards, embedding, hidden_1, hidden_2, output_dim)\n",
    "\n",
    "ALPHA = 0.0001          # Learning rate\n",
    "optimizer = optim.Adam(emb_model.parameters(), lr=ALPHA)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise buffer\n",
    "buffer = NN.ReplayBufferEmbedding(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "#torch.save(emb_model.state_dict(), \"embedded.pth\")# commented so i dont accidentally override my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neomi\\AppData\\Local\\Temp\\ipykernel_15836\\4132919009.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  emb_model.load_state_dict(torch.load(\"embedded.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load model\n",
    "emb_model.load_state_dict(torch.load(\"embedded.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different trainings: high EPSILON - low TRESHOLD, low EPSILON - high TRESHOLD, set to train\n",
    "late game or early game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training0\n",
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n",
      "training1\n",
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n",
      "training2\n",
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n",
      "training3\n",
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n",
      "training4\n",
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n",
      "training5\n",
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n",
      "training6\n",
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n",
      "training7\n",
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n",
      "training8\n",
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n",
      "training9\n",
      "Training complete!\n",
      "Training complete!\n",
      "Training complete!\n",
      "1465.5833735466003\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "# Hyperparameters, epsilon control is important\n",
    "num_of_games = 10000\n",
    "EPSILON = 0          # Exploration probability\n",
    "GAMMA = 0.9          # Discount factor\n",
    "\n",
    "trainings = 10\n",
    "\n",
    "start = time.time()\n",
    "for i in range(trainings):\n",
    "    print(f\"training{i}\")\n",
    "    train_model(emb_model, emb_target_model, optimizer, loss_fn, num_of_games, buffer, EPSILON, 2)\n",
    "    train_model(emb_model, emb_target_model, optimizer, loss_fn, num_of_games, buffer,  EPSILON, 1)\n",
    "    train_model(emb_model, emb_target_model, optimizer, loss_fn, num_of_games * 2, buffer, EPSILON, 0)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Budget: -4159.0\n",
      "Total Betted: 29095\n",
      "Folded: 906-times\n",
      "Betted 4x: 0-times\n",
      "Betted 2x: 1-times\n",
      "Betted 1x: 9093-times\n"
     ]
    }
   ],
   "source": [
    "# testing the model\n",
    "NN.testing_embedding(dqn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NSU25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
