{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN-4_13_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  represent cards as (suit, rank)\n",
    "\n",
    "Layers size: 15 -> [128, 64] -> 2\n",
    "\n",
    "batch size: 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random \n",
    "import ultimate_for_NN as ultimate\n",
    "from collections import deque\n",
    "import NN_functions as NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce GTX 1650 Ti\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # Should return True if a GPU is available\n",
    "print(torch.cuda.device_count())  # Number of GPUs available\n",
    "print(torch.cuda.get_device_name(0))  # Name of the first GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_1, hidden_2, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for training the model\n",
    "def train_model(model, target_model, optimizer, loss_fn, num_of_games, buffer,\n",
    "                EPSILON = .2, GAMMA = .9, starting_round = 0,\n",
    "                batch_size = 32, target_update = 500, train_freq = 4):\n",
    "    \n",
    "    def update_model_weights():\n",
    "        if buffer.size() < batch_size:  \n",
    "            return\n",
    "        \n",
    "        # Sample a mini-batch\n",
    "        actions, rewards, round_input_tensors, next_round_input_tensors, end = buffer.sample(batch_size)\n",
    " \n",
    "        # Convert to PyTorch tensors\n",
    "        actions = torch.tensor(actions, dtype=torch.int64)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        round_input_tensors = torch.tensor(round_input_tensors, dtype=torch.float32)\n",
    "        next_round_input_tensors = torch.tensor(next_round_input_tensors, dtype=torch.float32)\n",
    "        end = torch.tensor(end, dtype=torch.float32)\n",
    "        # Compute Q-values for current states (only the taken actions)\n",
    "        q_values = model(round_input_tensors).squeeze(1).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute target Q-values using Bellman equation\n",
    "        target_q_values = rewards\n",
    "        with torch.no_grad():\n",
    "            next_q_values = target_model(next_round_input_tensors).squeeze(1).max(1)[0]  # Max Q-value for next state\n",
    "            target_q_values += (GAMMA * next_q_values * (1 - end))  # Q-learning update, if round ended no future is included\n",
    "\n",
    "        # Compute loss (Mean Squared Error loss)\n",
    "        loss = loss_fn(q_values, target_q_values)\n",
    "\n",
    "        # Backpropagation & gradient update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Set model to train mode\n",
    "    model.train()\n",
    "\n",
    "    # Copy weights from dqn to target_model\n",
    "    target_model.load_state_dict(model.state_dict())\n",
    "    target_model.eval()  # No gradient updates for target\n",
    "\n",
    "    for episode in range(num_of_games):\n",
    "\n",
    "        if episode % target_update == 0:\n",
    "            target_model.load_state_dict(model.state_dict())  # Sync weights for target model\n",
    "        \n",
    "        if episode % train_freq == 0:  # Train every 4 steps\n",
    "            update_model_weights()\n",
    "\n",
    "        round = starting_round  # Start at Round 1\n",
    "        done = False\n",
    "        \n",
    "        # generate a training game\n",
    "        whole_game = NN.generate_game()\n",
    "\n",
    "        while not done:\n",
    "            # get input data for this round\n",
    "            round_input_tensor = NN.state_to_tensor(round, whole_game)\n",
    "            end = 0\n",
    "\n",
    "            # Epsilon-greedy action selection, we will explore with probability EPSILON\n",
    "            if np.random.rand() < EPSILON:\n",
    "                action = np.random.choice([0, 1])\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values = model(round_input_tensor)\n",
    "                    action = q_values.argmax().item()\n",
    "\n",
    "            # Calculate reward based on state and action, if action is check, reward is 0\n",
    "            reward = NN.reward_function(round, whole_game, action)  \n",
    "            \n",
    "            if action == 1:\n",
    "                round += 1 # move to the next state if the action is check/fold\n",
    "            \n",
    "            if action == 0 or round == 3:\n",
    "                end = 1\n",
    "                \n",
    "            # Determine the next state, if state == 3 it will be irrelevant\n",
    "            next_round_input_tensor = NN.state_to_tensor(round, whole_game)\n",
    "            \n",
    "            # add game to buffer\n",
    "            buffer.add(action, reward, round_input_tensor, next_round_input_tensor, end)\n",
    "           \n",
    "            # Transition to next state or end the episode if terminal\n",
    "            if  round == 3 or action == 0:\n",
    "                done = True\n",
    "\n",
    "        #if episode % 100 == 0:\n",
    "        #    print(f\"Episode {episode}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the model\n",
    "# layer dimensions\n",
    "input_dim = 15  # 1 for game state [0, 1, 2] and 7 for cards (1 card: (1-4, 1-13)), 0 means not known\n",
    "hidden_1 = 252 # hidden layer 1 size\n",
    "hidden_2 = 128 # hidden layer 2 size\n",
    "output_dim = 2  # Two actions: Bet (0) and Check/Fold (1)\n",
    "\n",
    "# Initialize DQN\n",
    "dqn = DQN(input_dim, hidden_1, hidden_2, output_dim)\n",
    "# Initialize DQN for target rewards, it will lag behind \n",
    "targetDQN = DQN(input_dim, hidden_1, hidden_2, output_dim)\n",
    " \n",
    "# try and get it on\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise buffer\n",
    "buffer = NN.ReplayBuffer(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "# Hyperparameters\n",
    "num_of_games = 10000\n",
    "EPSILON = 0.2         # Exploration probability\n",
    "ALPHA = 0.0001          # Learning rate\n",
    "GAMMA = 0.9          # Discount factor\n",
    "optimizer = optim.Adam(dqn.parameters(), lr=ALPHA)\n",
    "loss_fn = nn.MSELoss()\n",
    "trainings = 5\n",
    "for i in range(trainings):\n",
    "    print(f\"training{i}\")\n",
    "    train_model(dqn, targetDQN, optimizer, loss_fn, num_of_games, buffer, EPSILON, GAMMA, 2)\n",
    "    train_model(dqn, targetDQN, optimizer, loss_fn, num_of_games, buffer, EPSILON, GAMMA, 1)\n",
    "    train_model(dqn, targetDQN, optimizer, loss_fn, num_of_games, buffer, EPSILON, GAMMA, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NSU25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
