{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network - v 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3 rounds.\n",
    "- 1. Bet or check\n",
    "- 2. Bet or check\n",
    "- 3. Bet or fold\n",
    "\n",
    "We generate the game first and then feed the data to network,\n",
    "so we know the game rewards.\\\\\n",
    "\n",
    "Learning parameters:\n",
    "- epsilon for exploration vs exploitation\n",
    "- gama for future rewards\n",
    "- learning rate for learning intensity\n",
    "\n",
    "We implement Buffer and learning with batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random \n",
    "import ultimate\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define card set\n",
    "suits = ['Hearts', 'Diamonds', 'Clubs', 'Spades']\n",
    "ranks = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\n",
    "rank_values = {rank: i for i, rank in enumerate(ranks, start=2)}\n",
    "\n",
    "deck = [{'rank': rank, 'suit': suit} for suit in suits for rank in ranks]\n",
    "\n",
    "combinations = [\"High Card\", \"One Pair\", \"Two Pair\", \"Three of a Kind\", \"Four of a Kind\", \n",
    "                \"Full House\", \"Straight\", \"Flush\", \"Straight Flush\", \"Royal Flush\"]\n",
    "combinations_values = {combination: i for i, combination in enumerate(combinations, start=1)}\n",
    "# set ordered winning combinations\n",
    "winning_hands = [\"High Card\", \"One Pair\", \"Two Pair\", \"Three of a Kind\", \"Straight\", \"Flush\", \n",
    "                \"Full House\", \"Four of a Kind\", \"Straight Flush\", \"Royal Flush\"]\n",
    "#enumerate the deck\n",
    "enumerated_deck = dict(enumerate(deck, start=1))\n",
    "num_deck = np.arange(1, 53)\n",
    "\n",
    "# define split card dict, num_of_cards: (num of color, num of rank) ex. 13: (1, 13)\n",
    "split_card_dict={}\n",
    "for i in num_deck:\n",
    "    split_card_dict[i] = ((i - 1) // 13 + 1, (i-1) % 13 + 1)\n",
    "\n",
    "split_card_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for generating a game (all cards) - output a list of all cards in game cards\n",
    "# cards are interpreted as: 2 for each player, 5 for the table, last 2 for the dealer\n",
    "def generate_game(num_of_players = 1):\n",
    "    game_size = 7 + 2*num_of_players\n",
    "\n",
    "    whole_game = np.random.choice(np.arange(1, 53), size=game_size, replace=False)\n",
    "    \n",
    "    return whole_game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_1, hidden_2, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next state and next cards - returns next state input, game is 7-14 with split info for cards\n",
    "# each cards ha info about suit (1-4) and rank (1-13)\n",
    "#hardcoded for one player\n",
    "def state_to_tensor(round, whole_game):\n",
    "    split_card_info = [element for i in whole_game for element in split_card_dict[i]]\n",
    "\n",
    "    cards_len = len(split_card_info) - 4\n",
    "    if round == 0:\n",
    "        cards = np.array(split_card_info[:4])\n",
    "    elif round == 1:\n",
    "        cards = np.array(split_card_info[:10])\n",
    "    elif round == 2:\n",
    "        cards = np.array(split_card_info[:14])\n",
    "    else:\n",
    "        cards = np.array([]) # irrelevant just for filler\n",
    "\n",
    "    cards = np.pad(cards, (0, cards_len - len(cards)), mode='constant')\n",
    "    state = np.array([round])\n",
    "    full_state = np.concatenate([state, cards])\n",
    "    return torch.from_numpy(full_state).unsqueeze(0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward function - target is where the reward was biggest\n",
    "def reward_function(round, whole_game, action):\n",
    "    # normalize reward values (royal flush and straight flusch \n",
    "    # will be more than 1 but its tolerable)\n",
    "    \n",
    "    def normalize_reward(r, min_r=-6, max_r=11):\n",
    "        return (r - min_r) / (max_r - min_r)\n",
    "    # blind and ante are set to be 1\n",
    "    blind = 1\n",
    "    ante = 1\n",
    "\n",
    "    # if we fold in third round - we lose \n",
    "    if round == 2 and action == 1:\n",
    "        return normalize_reward(- blind - ante) \n",
    "    # if we say check in first or second round - the reward is neutral \n",
    "    elif round != 2 and action == 1:\n",
    "        return normalize_reward(0)\n",
    "    \n",
    "    # in all other cases we need to know who won and if the ante or blind are valid\n",
    "    player_hand = [enumerated_deck[card] for card in whole_game[:7]]\n",
    "    dealer_hand = [enumerated_deck[card] for card in whole_game[2:]]\n",
    "    #print(player_hand)\n",
    "    #print(dealer_hand)\n",
    "    player_combination = ultimate.get_best_hand(player_hand)\n",
    "    dealer_combination = ultimate.get_best_hand(dealer_hand)\n",
    "    #print(player_combination, dealer_combination)\n",
    "    # find the victor (Player, Dealer, Tie)\n",
    "    victor = \"\"\n",
    "    if winning_hands.index(player_combination) > winning_hands.index(dealer_combination):\n",
    "        victor = \"Player\"\n",
    "    elif winning_hands.index(player_combination) == winning_hands.index(dealer_combination):\n",
    "        result = ultimate.decider(player_combination, player_hand, \n",
    "                                  dealer_combination, dealer_hand)\n",
    "        if result == \"player\":\n",
    "            victor = \"Player\"\n",
    "        elif result == \"dealer\":\t\n",
    "            victor = \"Dealer\"\t\n",
    "        else:\n",
    "            return normalize_reward(0) # the reward for game is 0 if tie\n",
    "    else:\n",
    "        victor = \"Dealer\"\n",
    "\n",
    "    # we bet in third round\n",
    "    if round == 2:\n",
    "        if victor == \"Player\":\n",
    "            # calculate ante and blind\n",
    "            ante_valid = ultimate.has_ante(dealer_hand, dealer_combination) # boolean\n",
    "            blind_pay = ultimate.net_blind_payout(blind, player_combination) # value if won\n",
    "            return normalize_reward(ante + blind_pay + ante if ante_valid else 0)\n",
    "        else:\n",
    "            return normalize_reward(- blind - 2*ante)\n",
    "    # we bet in second round\n",
    "    elif round == 1:\n",
    "        if victor == \"Player\":\n",
    "            # calculate ante and blind\n",
    "            ante_valid = ultimate.has_ante(dealer_hand, dealer_combination) # boolean\n",
    "            blind_pay = ultimate.net_blind_payout(blind, player_combination) # value if won\n",
    "            return normalize_reward(2 * ante + blind_pay + ante if ante_valid else 0)\n",
    "        else:\n",
    "            return normalize_reward(- blind - 3*ante)\n",
    "    # we bet in first round\n",
    "    elif round == 0:\n",
    "        if victor == \"Player\":\n",
    "            # calculate ante and blind\n",
    "            ante_valid = ultimate.has_ante(dealer_hand, dealer_combination) # boolean\n",
    "            blind_pay = ultimate.net_blind_payout(blind, player_combination) # value if won\n",
    "            return normalize_reward(4 * ante + blind_pay + ante if ante_valid else 0)\n",
    "        else:\n",
    "            return normalize_reward(- blind - 5*ante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)  # Stores experiences up to `capacity`\n",
    "\n",
    "    # store games in buffer\n",
    "    def add(self, action, reward, round_input_tensor, next_round_input_tensor, end):\n",
    "        self.buffer.append((action, reward, round_input_tensor, next_round_input_tensor, end))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)  # Random mini-batch\n",
    "        actions, rewards, round_input_tensors, next_round_input_tensors, end = zip(*batch)\n",
    "        \n",
    "        # Convert to NumPy arrays for easier tensor conversion\n",
    "        return (np.array(actions), np.array(rewards), np.array(round_input_tensors),\n",
    "                np.array(next_round_input_tensors), np.array(end))\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)  # Current number of experiences stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for training the model\n",
    "def train_model(model, target_model, optimizer, loss_fn, num_of_games, buffer,\n",
    "                EPSILON = .2, GAMMA = .9, starting_round = 0,\n",
    "                batch_size = 32, target_update = 500, train_freq = 4):\n",
    "    \n",
    "    def update_model_weights():\n",
    "        if buffer.size() < batch_size:  \n",
    "            return\n",
    "        \n",
    "        # Sample a mini-batch\n",
    "        actions, rewards, round_input_tensors, next_round_input_tensors, end = buffer.sample(batch_size)\n",
    " \n",
    "        # Convert to PyTorch tensors\n",
    "        actions = torch.tensor(actions, dtype=torch.int64)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        round_input_tensors = torch.tensor(round_input_tensors, dtype=torch.float32)\n",
    "        next_round_input_tensors = torch.tensor(next_round_input_tensors, dtype=torch.float32)\n",
    "        end = torch.tensor(end, dtype=torch.float32)\n",
    "        print(\"Model output shape:\", model(round_input_tensors).shape)\n",
    "        print(\"Actions shape before unsqueeze:\", actions.shape)\n",
    "        print(\"Actions shape after unsqueeze:\", actions.unsqueeze(1).shape)\n",
    "        # Compute Q-values for current states (only the taken actions)\n",
    "        q_values = model(round_input_tensors).squeeze(1).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute target Q-values using Bellman equation\n",
    "        target_q_values = rewards\n",
    "        with torch.no_grad():\n",
    "\n",
    "            next_q_values = target_model(next_round_input_tensors).max(1)[0]  # Max Q-value for next state\n",
    "\n",
    "            print(\"Before squeeze:\", target_model(next_round_input_tensors).shape)  \n",
    "            print(\"After squeeze:\", target_model(next_round_input_tensors).squeeze(1).shape) \n",
    "            print(target_q_values.shape, next_q_values.shape, end.shape)\n",
    "            print(next_q_values)\n",
    "            target_q_values += (0.99 * next_q_values * (1 - end))  # Q-learning update, if round ended no future is included\n",
    "\n",
    "        # Compute loss (Mean Squared Error loss)\n",
    "        loss = loss_fn(q_values, target_q_values)\n",
    "\n",
    "        # Backpropagation & gradient update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Set model to train mode\n",
    "    model.train()\n",
    "\n",
    "    # Copy weights from dqn to target_model\n",
    "    target_model.load_state_dict(model.state_dict())\n",
    "    target_model.eval()  # No gradient updates for target\n",
    "\n",
    "    for episode in range(num_of_games):\n",
    "\n",
    "        if episode % target_update == 0:\n",
    "            target_model.load_state_dict(model.state_dict())  # Sync weights for target model\n",
    "        \n",
    "        if episode % train_freq == 0:  # Train every 4 steps\n",
    "            update_model_weights()\n",
    "\n",
    "        round = starting_round  # Start at Round 1\n",
    "        done = False\n",
    "        \n",
    "        # generate a training game\n",
    "        whole_game = generate_game()\n",
    "\n",
    "        while not done:\n",
    "            # get input data for this round\n",
    "            round_input_tensor = state_to_tensor(round, whole_game)\n",
    "            end = 0\n",
    "\n",
    "            # Epsilon-greedy action selection, we will explore with probability EPSILON\n",
    "            if np.random.rand() < EPSILON:\n",
    "                action = np.random.choice([0, 1])\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values = model(round_input_tensor)\n",
    "                    action = q_values.argmax().item()\n",
    "\n",
    "            # Calculate reward based on state and action, if action is check, reward is 0\n",
    "            reward = reward_function(round, whole_game, action)  \n",
    "            \n",
    "            if action == 1:\n",
    "                round += 1 # move to the next state if the action is check/fold\n",
    "            \n",
    "            if action == 0 or round == 3:\n",
    "                end = 1\n",
    "                \n",
    "            # Determine the next state, if state == 3 it will be irrelevant\n",
    "            next_round_input_tensor = state_to_tensor(round, whole_game)\n",
    "            \n",
    "            # add game to buffer\n",
    "            buffer.add(action, reward, round_input_tensor, next_round_input_tensor, end)\n",
    "           \n",
    "            # Transition to next state or end the episode if terminal\n",
    "            if  round == 3 or action == 0:\n",
    "                done = True\n",
    "\n",
    "        #if episode % 100 == 0:\n",
    "        #    print(f\"Episode {episode}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the model\n",
    "# layer dimensions\n",
    "input_dim = 15  # 1 for game state [0, 1, 2] and 7 for cards (1 card: (1-4, 1-13)), 0 means not known\n",
    "hidden_1 = 252 # hidden layer 1 size\n",
    "hidden_2 = 128 # hidden layer 2 size\n",
    "output_dim = 2  # Two actions: Bet (0) and Check/Fold (1)\n",
    "\n",
    "# Initialize DQN\n",
    "dqn = DQN(input_dim, hidden_1, hidden_2, output_dim)\n",
    "# Initialize DQN for target rewards, it will lag behind \n",
    "targetDQN = DQN(input_dim, hidden_1, hidden_2, output_dim)\n",
    " \n",
    "# try and get it on\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise buffer\n",
    "buffer = ReplayBuffer(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training0\n",
      "Model output shape: torch.Size([32, 1, 2])\n",
      "Actions shape before unsqueeze: torch.Size([32])\n",
      "Actions shape after unsqueeze: torch.Size([32, 1])\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32]) torch.Size([32, 2]) torch.Size([32])\n",
      "tensor([[-0.6350, -0.6722],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [-1.1807, -0.6817],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [-0.5371, -0.9403],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [-0.4315, -0.7774],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [ 0.0299, -0.0309],\n",
      "        [ 0.0299, -0.0309]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (32) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trainings):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m     train_model(dqn, targetDQN, optimizer, loss_fn, num_of_games, buffer, EPSILON, GAMMA, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     13\u001b[0m     train_model(dqn, targetDQN, optimizer, loss_fn, num_of_games, buffer, EPSILON, GAMMA, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m     train_model(dqn, targetDQN, optimizer, loss_fn, num_of_games, buffer, EPSILON, GAMMA, \u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[95], line 56\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, target_model, optimizer, loss_fn, num_of_games, buffer, EPSILON, GAMMA, starting_round, batch_size, target_update, train_freq)\u001b[0m\n\u001b[0;32m     53\u001b[0m     target_model\u001b[38;5;241m.\u001b[39mload_state_dict(model\u001b[38;5;241m.\u001b[39mstate_dict())  \u001b[38;5;66;03m# Sync weights for target model\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m train_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Train every 4 steps\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     update_model_weights()\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mround\u001b[39m \u001b[38;5;241m=\u001b[39m starting_round  \u001b[38;5;66;03m# Start at Round 1\u001b[39;00m\n\u001b[0;32m     59\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[95], line 33\u001b[0m, in \u001b[0;36mtrain_model.<locals>.update_model_weights\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(target_q_values\u001b[38;5;241m.\u001b[39mshape, next_q_values\u001b[38;5;241m.\u001b[39mshape, end\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(next_q_values)\n\u001b[1;32m---> 33\u001b[0m     target_q_values \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0.99\u001b[39m \u001b[38;5;241m*\u001b[39m next_q_values \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m end))  \u001b[38;5;66;03m# Q-learning update, if round ended no future is included\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Compute loss (Mean Squared Error loss)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(q_values, target_q_values)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (32) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "# Hyperparameters\n",
    "num_of_games = 1000\n",
    "EPSILON = 0.2         # Exploration probability\n",
    "ALPHA = 0.0001          # Learning rate\n",
    "GAMMA = 0.9          # Discount factor\n",
    "optimizer = optim.Adam(dqn.parameters(), lr=ALPHA)\n",
    "loss_fn = nn.MSELoss()\n",
    "trainings = 1\n",
    "for i in range(trainings):\n",
    "    print(f\"training{i}\")\n",
    "    train_model(dqn, targetDQN, optimizer, loss_fn, num_of_games, buffer, EPSILON, GAMMA, 2)\n",
    "    train_model(dqn, targetDQN, optimizer, loss_fn, num_of_games, buffer, EPSILON, GAMMA, 1)\n",
    "    train_model(dqn, targetDQN, optimizer, loss_fn, num_of_games, buffer, EPSILON, GAMMA, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2067, -0.2101]])\n",
      "Total Budget: 0.0\n",
      "Total Betted: 6\n",
      "Folded: 0-times\n",
      "Betted 4x: 1-times\n",
      "Betted 2x: 0-times\n",
      "Betted 1x: 0-times\n"
     ]
    }
   ],
   "source": [
    "# testing model\n",
    "dqn.eval()  # Set model to evaluation mode\n",
    "games = 1\n",
    "budget = 0\n",
    "betted = 0\n",
    "betted4x = 0\n",
    "betted2x = 0\n",
    "betted1x = 0\n",
    "folded = 0\n",
    "for i in range(games):\n",
    "    round = 0\n",
    "\n",
    "    whole_game = generate_game()\n",
    "    while True:\n",
    "        state_tensor = state_to_tensor(round, whole_game)\n",
    "         \n",
    "        with torch.no_grad():\n",
    "            q_values = dqn(state_tensor)\n",
    "            action = q_values.argmax().item()\n",
    "        print(q_values)\n",
    "        # if action is no (check/fold) we move to the next round\n",
    "        if action == 1 and round != 2:\n",
    "            round += 1\n",
    "        else:\n",
    "            budget += reward_function(round, whole_game, action)\n",
    "            if action == 1 and round == 2:\n",
    "                folded += 1\n",
    "                betted += 2 # currently hardcoded\n",
    "            elif round == 2:\n",
    "                betted1x += 1\n",
    "                betted += 3\n",
    "            elif round == 1:\n",
    "                betted2x += 1\n",
    "                betted += 4\n",
    "            elif round == 0:\n",
    "                betted4x += 1\n",
    "                betted += 6\n",
    "            break\n",
    "    \n",
    "    #print(f\"Game: {i+1}, Budget: {budget}\")\n",
    "\n",
    "print(f\"Total Budget: {budget}\")\n",
    "print(f\"Total Betted: {betted}\")\n",
    "print(f\"Folded: {folded}-times\")\n",
    "print(f\"Betted 4x: {betted4x}-times\")\n",
    "print(f\"Betted 2x: {betted2x}-times\")\n",
    "print(f\"Betted 1x: {betted1x}-times\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
